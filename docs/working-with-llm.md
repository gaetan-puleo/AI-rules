# Working with LLMs

## ðŸ“š Introduction

This guide provides practical insights into working effectively with large language models (LLMs), covering:

- What LLMs can (and can't) do  
- How to manage context efficiently  
- How to choose the right model for your use case


## ðŸ“– Understanding LLM Capabilities

### Context Management

When interacting with an LLM, its responses are driven by the context you provide â€” especially the recent conversation history. Interestingly, the **first tokens** in the context tend to carry more weight.

Each model has its own context window (i.e., the number of tokens it can process at once). For example, **Gemini Pro 2.5** supports up to 1 million tokens â€” but that doesn't mean you should use all of it.

Providing too much information leads to:
- **Slower responses**, because more tokens require more computation
- **Lower accuracy**, as the model may get distracted from the main task

To manage context effectively:
1. **Start a new conversation for each new task or feature** â€” donâ€™t carry over unrelated history.  
2. **Stay under 50,000 tokens** when possible â€” it keeps things fast and focused.  
3. **Avoid debating with the model** â€” itâ€™s not a person. If it misinterprets instructions, reset and try again with clearer input.

> A 2023 study showed LLMs perform best when key information is placed at the beginning or end of the context.  
> [ðŸ§  Understanding LLM Context Windows](https://medium.com/@tahirbalarabe2/understanding-llm-context-windows-tokens-attention-and-challenges-c98e140f174d)  
> [ðŸ“¹ Context Rot and LLM Performance](https://www.youtube.com/watch?v=TUjQuC4ugak)


### Instruction Following: Use the Right Models

When developing features, planning is critical â€” you need to define objectives, break tasks into steps, and follow conventions.  
But not all models are good at **instruction following**, which is what matters most in this workflow.

Popular benchmarks often focus on:
- Exercise solving
- General knowledge Q&A

These arenâ€™t always relevant. You donâ€™t need a model that can ace trivia or solve math problems â€” you need one that **follows instructions consistently and respects project structure**.

#### A Note on "Thinking Models"

â€œThinking modelsâ€ are designed to reason deeply, solve complex problems, and generate structured content. But for typical product development, they often overcomplicate things.

Hereâ€™s a counterintuitive insight:  
> **You donâ€™t always want a model that â€œthinks.â€**

Based on Appleâ€™s recent research and my own tests, models like **DeepSeek R1** and **Gemini Pro 2.5** tend to:
- Drift from instructions  
- Add unnecessary reasoning  
- Attempt to generate more than asked

For example, when prompted to create only a spec file, theyâ€™ll also try to generate implementation files â€” which breaks the workflow.

On the other hand, simpler models like **DeepSeek Chat V3** often outperform them for basic instruction-following tasks.

#### Recommendations:

1. **Use models optimized for following instructions.** Personally, I use **Claude Sonnet 4** for this.  
2. **Disable "thinking tokens" if possible** by setting the thinking budget to zero â€” this forces simpler, more deterministic behavior.  
3. **Smaller models may eventually outperform larger ones** for some tasks, though current evidence is still limited. Be cautious with their shorter context limits.

> "Their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. By comparing LRMs with their standard LLM counterparts under equivalent inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks where both models experience complete collapse."  
â€” [Apple: *The Illusion of Thinking*](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)

